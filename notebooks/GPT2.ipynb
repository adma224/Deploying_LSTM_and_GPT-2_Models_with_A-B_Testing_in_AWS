{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8385cdb2-d8a2-4bbe-9a41-99af85930c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import transformers\n",
    "from transformers import (\n",
    "    GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8920a22-47c9-4f9a-ab94-2c3df9bfbb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Set device ONCE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Memory optimizations (move these up)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.set_per_process_memory_fraction(0.6, device=0)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Limit GPU & CPU usage\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"  # Limit CPU threads\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb21099-2769-4592-9941-656f28ab8626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(\"../data/cleaned_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb4ed7e-3ed0-4db2-8806-79d3c5a9da25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207996 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...\n",
       "1       American Airlines Flyer Charged, Banned For Li...\n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...\n",
       "3       The Funniest Tweets From Parents This Week (Se...\n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...\n",
       "...                                                   ...\n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...\n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...\n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...\n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...\n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...\n",
       "\n",
       "[207996 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a3c30db-a4f2-4b68-9533-3bad9d61f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"headline\"])  # Remove rows where 'headline' is NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fa1208c-2487-4ed9-833a-7d91cdb8244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have padding by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c887ae-60ab-4a55-aec6-2660bc04d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"tokenized\"] = df[\"headline\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "max_length = max(len(tokens) for tokens in df[\"tokenized\"])\n",
    "\n",
    "df.loc[:, \"padded\"] = df[\"tokenized\"].apply(lambda x: x + [tokenizer.pad_token_id] * (max_length - len(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0065bb00-bb7d-44ad-90bf-2566aa67ff62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>padded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121815</th>\n",
       "      <td>Celebrities Emerging From Water Because Hey .....</td>\n",
       "      <td>[42741, 65, 19491, 48297, 3574, 5638, 4362, 14...</td>\n",
       "      <td>[42741, 65, 19491, 48297, 3574, 5638, 4362, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76633</th>\n",
       "      <td>Bill Maher Slams The Internet For Killing The ...</td>\n",
       "      <td>[17798, 38137, 30382, 82, 383, 4455, 1114, 255...</td>\n",
       "      <td>[17798, 38137, 30382, 82, 383, 4455, 1114, 255...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188697</th>\n",
       "      <td>The Most Expensive NFL Tickets Of The Season: ...</td>\n",
       "      <td>[464, 4042, 5518, 2021, 5134, 26878, 3226, 383...</td>\n",
       "      <td>[464, 4042, 5518, 2021, 5134, 26878, 3226, 383...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80361</th>\n",
       "      <td>Students Surprise Starbucks Employee With Gene...</td>\n",
       "      <td>[28239, 47893, 24527, 36824, 2080, 2980, 516, ...</td>\n",
       "      <td>[28239, 47893, 24527, 36824, 2080, 2980, 516, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65636</th>\n",
       "      <td>Ben Stein: 'I Don't Think Trump Knows A Goddam...</td>\n",
       "      <td>[11696, 15215, 25, 705, 40, 2094, 470, 11382, ...</td>\n",
       "      <td>[11696, 15215, 25, 705, 40, 2094, 470, 11382, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43356</th>\n",
       "      <td>Trump's \"Extreme Vetting\" Of Refugees Empowers...</td>\n",
       "      <td>[6170, 338, 366, 36716, 569, 35463, 1, 3226, 3...</td>\n",
       "      <td>[6170, 338, 366, 36716, 569, 35463, 1, 3226, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113206</th>\n",
       "      <td>For $6 You Can Give a Coffee Tree and Help Emp...</td>\n",
       "      <td>[1890, 720, 21, 921, 1680, 13786, 257, 19443, ...</td>\n",
       "      <td>[1890, 720, 21, 921, 1680, 13786, 257, 19443, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66542</th>\n",
       "      <td>Brad Paisley Debuts A Little Ditty About North...</td>\n",
       "      <td>[30805, 11243, 271, 1636, 1024, 4360, 82, 317,...</td>\n",
       "      <td>[30805, 11243, 271, 1636, 1024, 4360, 82, 317,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91949</th>\n",
       "      <td>Video Shows Man Holding Gun Before Allegedly S...</td>\n",
       "      <td>[10798, 25156, 1869, 31703, 6748, 7413, 26326,...</td>\n",
       "      <td>[10798, 25156, 1869, 31703, 6748, 7413, 26326,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117668</th>\n",
       "      <td>San Francisco Radio Stations Ban Lorde's 'Roya...</td>\n",
       "      <td>[15017, 6033, 8829, 520, 602, 10274, 4453, 68,...</td>\n",
       "      <td>[15017, 6033, 8829, 520, 602, 10274, 4453, 68,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "121815  Celebrities Emerging From Water Because Hey .....   \n",
       "76633   Bill Maher Slams The Internet For Killing The ...   \n",
       "188697  The Most Expensive NFL Tickets Of The Season: ...   \n",
       "80361   Students Surprise Starbucks Employee With Gene...   \n",
       "65636   Ben Stein: 'I Don't Think Trump Knows A Goddam...   \n",
       "...                                                   ...   \n",
       "43356   Trump's \"Extreme Vetting\" Of Refugees Empowers...   \n",
       "113206  For $6 You Can Give a Coffee Tree and Help Emp...   \n",
       "66542   Brad Paisley Debuts A Little Ditty About North...   \n",
       "91949   Video Shows Man Holding Gun Before Allegedly S...   \n",
       "117668  San Francisco Radio Stations Ban Lorde's 'Roya...   \n",
       "\n",
       "                                                tokenized  \\\n",
       "121815  [42741, 65, 19491, 48297, 3574, 5638, 4362, 14...   \n",
       "76633   [17798, 38137, 30382, 82, 383, 4455, 1114, 255...   \n",
       "188697  [464, 4042, 5518, 2021, 5134, 26878, 3226, 383...   \n",
       "80361   [28239, 47893, 24527, 36824, 2080, 2980, 516, ...   \n",
       "65636   [11696, 15215, 25, 705, 40, 2094, 470, 11382, ...   \n",
       "...                                                   ...   \n",
       "43356   [6170, 338, 366, 36716, 569, 35463, 1, 3226, 3...   \n",
       "113206  [1890, 720, 21, 921, 1680, 13786, 257, 19443, ...   \n",
       "66542   [30805, 11243, 271, 1636, 1024, 4360, 82, 317,...   \n",
       "91949   [10798, 25156, 1869, 31703, 6748, 7413, 26326,...   \n",
       "117668  [15017, 6033, 8829, 520, 602, 10274, 4453, 68,...   \n",
       "\n",
       "                                                   padded  \n",
       "121815  [42741, 65, 19491, 48297, 3574, 5638, 4362, 14...  \n",
       "76633   [17798, 38137, 30382, 82, 383, 4455, 1114, 255...  \n",
       "188697  [464, 4042, 5518, 2021, 5134, 26878, 3226, 383...  \n",
       "80361   [28239, 47893, 24527, 36824, 2080, 2980, 516, ...  \n",
       "65636   [11696, 15215, 25, 705, 40, 2094, 470, 11382, ...  \n",
       "...                                                   ...  \n",
       "43356   [6170, 338, 366, 36716, 569, 35463, 1, 3226, 3...  \n",
       "113206  [1890, 720, 21, 921, 1680, 13786, 257, 19443, ...  \n",
       "66542   [30805, 11243, 271, 1636, 1024, 4360, 82, 317,...  \n",
       "91949   [10798, 25156, 1869, 31703, 6748, 7413, 26326,...  \n",
       "117668  [15017, 6033, 8829, 520, 602, 10274, 4453, 68,...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subset = df.sample(n=100, random_state=42)  # Random 100 rows\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf4716-2732-4b57-9075-d08035a8d6bb",
   "metadata": {},
   "source": [
    "##### Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6b78f17-bab1-4bd9-82d5-e46243275944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, df_subset):\n",
    "        self.input_ids = torch.tensor(df_subset[\"padded\"].tolist(), dtype=torch.long)\n",
    "        self.attention_mask = (self.input_ids != tokenizer.pad_token_id).long()  # Mask padding tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.input_ids[idx],  # GPT-2 is trained using its own inputs as labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c714cf26-ae62-4571-a786-8db3dfd6d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation Split\n",
    "train_size = int(0.8 * len(df_subset))\n",
    "train_df, val_df = df_subset[:train_size], df_subset[train_size:]\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = GPT2Dataset(train_df)\n",
    "val_dataset = GPT2Dataset(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e27f5-a30a-4a85-b7ea-b32347d64e91",
   "metadata": {},
   "source": [
    "DistilGPT2 has 6 transformer blocks compared to GPT2, which has 12 transformer blocks. To perform transfer learning properly we will freeze all layers and unfreeze the last 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d6b72c0-57f4-4d2f-8204-c9d1e9e185d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/adma224/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/adma224/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/adma224/.cache/huggingface/hub/models--distilgpt2/snapshots/2290a62682d06624634c1f46a6ad5be0f47f38aa/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Pretrained GPT-2 Model with LM head\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")  # 50% smaller\n",
    "\n",
    "# Freeze all layers initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last 4 layers\n",
    "for param in model.transformer.h[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move Model to GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fbe5c-0753-497e-870e-defa8b9f724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0cbdb-3e15-4d7a-8808-a3c1ae096168",
   "metadata": {},
   "source": [
    "##### Choose a batch size and num workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5fc1028-e5e2-4fe1-a946-62d06d8334f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data loaders ready!\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=1, shuffle=True)  # Reduce num_workers\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, num_workers=1, shuffle=False)\n",
    "\n",
    "print(\"Model and data loaders ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1a45e60-1d43-46bd-a44f-14c1bdc885b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set Hugging Face Transformers library to show debug logs\n",
    "transformers.logging.set_verbosity_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03c348a2-6418-42e2-ac24-be7d2c6acbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Using device: cuda\n",
      "🚀 GPU Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "💾 GPU Memory Allocated: 1916.61 MB\n",
      "💾 GPU Memory Reserved: 3370.00 MB\n",
      "🔄 CUDA Version: 12.1\n"
     ]
    }
   ],
   "source": [
    "# Print selected device\n",
    "print(f\"🔥 Using device: {device}\")\n",
    "\n",
    "# Print GPU info\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"🚀 GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"💾 GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"🔄 CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"🖥 Running on CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f3048-0348-4804-b2c4-17d67134e677",
   "metadata": {},
   "source": [
    "Reducing Learning Rate\n",
    "Why? A high learning rate can cause large weight updates, leading to overwriting GPT-2’s pre-trained knowledge.\n",
    "How? Use a smaller learning rate than usual when fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e014f55-8bfc-4e86-8210-e87002b47232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Where model checkpoints will be saved\n",
    "    logging_dir=\"./logs\",  # Directory for logging\n",
    "    logging_strategy=\"steps\",  # Log at every step\n",
    "    logging_steps=50,  # Log every 50 steps\n",
    "    report_to=[\"tensorboard\"],  # Log to TensorBoard\n",
    "    eval_strategy=\"epoch\",  # Evaluate at each epoch\n",
    "    save_strategy=\"epoch\",  # Save model at each epoch\n",
    "    save_total_limit=5,  # Keep only last 2 checkpoints\n",
    "    disable_tqdm=False,  # Enable progress bars\n",
    "    load_best_model_at_end=True,  # Load best model checkpoint at end\n",
    "    fp16=True,  # Enable mixed precision for speed\n",
    "    per_device_train_batch_size=8,  # Adjust batch size to prevent memory issues\n",
    "    per_device_eval_batch_size=8,  # Same for evaluation\n",
    "    gradient_accumulation_steps=1,  # Accumulate gradients before updating weights\n",
    "    learning_rate=5e-5, # Lower than usual (default is 5e-4)\n",
    "    weight_decay=0.01,  # Prevent drastic weight changes\n",
    "    num_train_epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73bbf9f5-e19e-4431-bcd8-93452750393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer.Trainer object at 0x7ff925bb3d60>\n"
     ]
    }
   ],
   "source": [
    "# Use Hugging Face Trainer API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "print(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e9e0b15-d114-40cd-929b-20fa08079cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30\n",
      "  Number of trainable parameters = 81,912,576\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.286885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.521786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.511022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-10\n",
      "Configuration saved in ./results/checkpoint-10/config.json\n",
      "Configuration saved in ./results/checkpoint-10/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-10/model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-20\n",
      "Configuration saved in ./results/checkpoint-20/config.json\n",
      "Configuration saved in ./results/checkpoint-20/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-20/model.safetensors\n",
      "Saving model checkpoint to ./results/checkpoint-30\n",
      "Configuration saved in ./results/checkpoint-30/config.json\n",
      "Configuration saved in ./results/checkpoint-30/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-30/model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/checkpoint-30\n",
      "Configuration saved in ./results/checkpoint-30/config.json\n",
      "Configuration saved in ./results/checkpoint-30/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-30/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-30 (score: 0.5110220909118652).\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "# Measure start time\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"🚀 Training started...\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Measure end time\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "432f471a-d540-4643-923e-50b18cbc1779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training completed in 41.57 seconds (0.69 minutes)\n",
      "📈 Final Epoch: 3.0\n",
      "📊 Total Training Steps: 30\n"
     ]
    }
   ],
   "source": [
    "training_duration = end_time - start_time\n",
    "# Print training duration\n",
    "print(f\"✅ Training completed in {training_duration:.2f} seconds ({training_duration/60:.2f} minutes)\")\n",
    "\n",
    "# Print final training state\n",
    "print(f\"📈 Final Epoch: {trainer.state.epoch}\")\n",
    "print(f\"📊 Total Training Steps: {trainer.state.global_step}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f8a5a-2515-46c5-868d-b3722c8c099e",
   "metadata": {},
   "source": [
    "##### Run this command in the terminal for training metrics in tensorboard\n",
    "\n",
    "`tensorboard --logdir=./logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46ca0ac8-8dc8-4724-8ced-0e35501cc4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ../models/gpt2_finetuned/config.json\n",
      "Configuration saved in ../models/gpt2_finetuned/generation_config.json\n",
      "Model weights saved in ../models/gpt2_finetuned/model.safetensors\n",
      "tokenizer config file saved in ../models/gpt2_finetuned/tokenizer_config.json\n",
      "Special tokens file saved in ../models/gpt2_finetuned/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save model & tokenizer\n",
    "model.save_pretrained(\"../models/gpt2_finetuned\")\n",
    "tokenizer.save_pretrained(\"../models/gpt2_finetuned\")\n",
    "\n",
    "print(\"Model saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "796120ec-db98-4cca-9fd8-c004c6eeed89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Evaluation Loss: 0.5110\n",
      "🔢 Perplexity: 1.6670\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "\n",
    "# Define function to calculate perplexity\n",
    "def compute_perplexity(eval_loss):\n",
    "    return np.exp(eval_loss)  # Perplexity = exp(loss)\n",
    "\n",
    "# Get evaluation loss from trainer\n",
    "eval_results = trainer.evaluate()\n",
    "eval_loss = eval_results[\"eval_loss\"]\n",
    "perplexity = compute_perplexity(eval_loss)\n",
    "\n",
    "print(f\"📝 Evaluation Loss: {eval_loss:.4f}\")\n",
    "print(f\"🔢 Perplexity: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a4fe578-c321-49c4-aece-46595f5463c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnWhitespace(StoppingCriteria):\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Stop generation if the last 5 tokens are whitespace\n",
    "        if all(tokenizer.decode(tok).isspace() for tok in input_ids[0, -5:]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnWhitespace()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4acff34e-359c-4243-85b2-d37316530fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Input: Breaking news:\n",
      "🔮 Output: Breaking news: Obama won't vote and that the GOP wants him to fight 'bailing out' Republican officials\n",
      "\n",
      "📝 Input: Latest update:\n",
      "🔮 Output: Latest update: A more detailed guide to learning the science and maths skills so that you know how they use all your knowledge on the go,\" he said\n",
      "\n",
      "📝 Input: The president announced that\n",
      "🔮 Output: The president announced that he won't meet with Congress on Tuesday\" and added, \"This was something I had very difficult time finding.\n",
      "\n",
      "📝 Input: Scientists have discovered a new\n",
      "🔮 Output: Scientists have discovered a new study in human sexual reproduction based on their sex lives for generations: how many women live each day - or how much they'll let themselves know what it is, even during the years before they die to avoid unwanted pregnancies.\"\n",
      "\n",
      "📝 Input: Experts warn that\n",
      "🔮 Output: Experts warn that it may be worth doing more for health and security if you do everything possible with it\n",
      "\n",
      "📝 Input: A recent study suggests that\n",
      "🔮 Output: A recent study suggests that it's less likely to kill a human who uses drugs without first getting on a plane\n",
      "\n",
      "📝 Input: Authorities have confirmed that\n",
      "🔮 Output: Authorities have confirmed that two civilians have been charged at St-John‌ Saint John Catholic Church. The latter will remain hospitalized on Saturday due to poor weather as her family says they can barely breathe through the pews before returning after 12 months of waiting and being given a month‬ ‬ for the first time (a year in February)\n",
      "\n",
      "📝 Input: In a surprising turn of events,\n",
      "🔮 Output: In a surprising turn of events, Obama's administration announced this week that they had halted work authorization for transgender members of the nation’ The only real reason for these actions? Why not be so vocal about their opposition to allowing transgender children to identify.\n",
      "\n",
      "📝 Input: The stock market responded to\n",
      "🔮 Output: The stock market responded to the news on Thursday night saying that a new law proposed by Sen. Al Franken may not \"get any federal support for President Trump's tax return,\" citing multiple senators.\n",
      "\n",
      "📝 Input: New regulations require companies to\n",
      "🔮 Output: New regulations require companies to provide financial statements on information that is relevant to the sale or promotion.‒\n",
      "\n",
      "📝 Input: Health officials recommend that\n",
      "🔮 Output: Health officials recommend that patients have an emergency contraception when the hormone replacement therapy will work better than in-patient and treatment for a heart attack\n",
      "\n",
      "📝 Input: Technology companies are investing in\n",
      "🔮 Output: Technology companies are investing in their new products to help them navigate an industry they have been through so many, often difficult days.\n",
      "\n",
      "📝 Input: Sports fans are excited about\n",
      "🔮 Output: Sports fans are excited about an update of the Galaxy S II's Touchscreen Widget Reader. This was launched in April 2016 and has now been implemented with a more accurate, multi-touch feature called ‪‹\n",
      "\n",
      "📝 Input: The weather forecast predicts\n",
      "🔮 Output: The weather forecast predicts about 3.6 miles of rain with a chance of falling to the south east by March, bringing total wind speed up to 17 mpg in the first 10 days — 2.25 miles above schedule for October. The storm was predicted on a 1 degree day (13.2km), down from last week's worst forecast, with a full month of fog at 30-40 °F [18]\n",
      "\n",
      "📝 Input: Researchers at MIT have developed\n",
      "🔮 Output: Researchers at MIT have developed an intelligent robotic robot for cleaning the toilet but still can't make soap themselves!\n",
      "\n",
      "📝 Input: Protests erupted in the city over\n",
      "🔮 Output: Protests erupted in the city over the killings of journalists in March 2012 and on Tuesday they threatened retaliation by Israeli authorities in an escalating attack that saw dozens killed. An attorney for Mr Netanyahu has said \"all those people involved should stay outside Israel if they continue to use them\".\n",
      "\n",
      "📝 Input: The Supreme Court ruled that\n",
      "🔮 Output: The Supreme Court ruled that the use of condoms outside a condom-wearing person constitutes a civil breach of their religious liberty\".\n",
      "\n",
      "📝 Input: Celebrities are reacting to\n",
      "🔮 Output: Celebrities are reacting to the negative reception that their favorite celebrities should be 'outrageous about Trump's health policies,' by tweeting this story:\n",
      "\n",
      "📝 Input: A new breakthrough in medicine shows that\n",
      "🔮 Output: A new breakthrough in medicine shows that patients can't treat a disease for years,\" says Kelli Hahnman\n",
      "\n",
      "📝 Input: The United Nations has issued a statement on\n",
      "🔮 Output: The United Nations has issued a statement on the ongoing crisis involving refugees' asylum-seekers being transferred from India during the migration crisis in 2015.\"It is clear that many have no reason for fear.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, top_p=0.9, top_k=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        max_length=100,\n",
    "        min_length=20,\n",
    "        do_sample=True,\n",
    "        num_beams=10,\n",
    "        length_penalty=1,\n",
    "        early_stopping=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Ensure it doesn't stop early due to padding\n",
    "        temperature=10.1,  # Increase randomness\n",
    "        top_p=top_p,  # Nucleus sampling (limits probability mass)\n",
    "        top_k=top_k,  # Only consider the top-k most likely words\n",
    "        repetition_penalty=2.1,  # Avoid repeated spaces\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).strip()  # Strip extra spaces\n",
    "\n",
    "# Test with new settings\n",
    "examples = [\n",
    "    \"Breaking news:\", \n",
    "    \"Latest update:\", \n",
    "    \"The president announced that\",\n",
    "    \"Scientists have discovered a new\",\n",
    "    \"Experts warn that\",\n",
    "    \"A recent study suggests that\",\n",
    "    \"Authorities have confirmed that\",\n",
    "    \"In a surprising turn of events,\",\n",
    "    \"The stock market responded to\",\n",
    "    \"New regulations require companies to\",\n",
    "    \"Health officials recommend that\",\n",
    "    \"Technology companies are investing in\",\n",
    "    \"Sports fans are excited about\",\n",
    "    \"The weather forecast predicts\",\n",
    "    \"Researchers at MIT have developed\",\n",
    "    \"Protests erupted in the city over\",\n",
    "    \"The Supreme Court ruled that\",\n",
    "    \"Celebrities are reacting to\",\n",
    "    \"A new breakthrough in medicine shows that\",\n",
    "    \"The United Nations has issued a statement on\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    print(f\"📝 Input: {text}\")\n",
    "    print(f\"🔮 Output: {generate_text(text)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc8214ce-464f-4fc1-990c-61047fc6ed42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Encoded Input: tensor([[29449,  1705,    25]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Breaking news:\", return_tensors=\"pt\").to(device)\n",
    "print(\"🔎 Encoded Input:\", input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ba46cca-d050-4be2-a0c2-8f21754e3318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Starting prompt: The United Nations has issued a statement on\n",
      "\n",
      "🔍 Token step 1\n",
      "   the: 0.2751\n",
      "   its: 0.0788\n",
      "   Tuesday: 0.0654\n",
      "   Monday: 0.0595\n",
      "   Wednesday: 0.0525\n",
      "\n",
      "🔍 Token step 2\n",
      "   situation: 0.0397\n",
      "   issue: 0.0248\n",
      "   matter: 0.0193\n",
      "   crisis: 0.0193\n",
      "   ongoing: 0.0176\n",
      "\n",
      "🔍 Token step 3\n",
      "   in: 0.4893\n",
      "  ,: 0.0774\n",
      "   and: 0.0642\n",
      "  .: 0.0603\n",
      "   with: 0.0402\n",
      "\n",
      "🔍 Token step 4\n",
      "   Syria: 0.4355\n",
      "   Yemen: 0.0858\n",
      "   Ukraine: 0.0474\n",
      "   Iraq: 0.0431\n",
      "   Gaza: 0.0405\n",
      "\n",
      "🔍 Token step 5\n",
      "  ,: 0.2934\n",
      "   and: 0.1149\n",
      "  .: 0.1079\n",
      "  :: 0.0765\n",
      "   that: 0.0526\n",
      "\n",
      "🔍 Token step 6\n",
      "   saying: 0.1526\n",
      "   calling: 0.1117\n",
      "   and: 0.0636\n",
      "   which: 0.0465\n",
      "   urging: 0.0465\n",
      "\n",
      "🔍 Token step 7\n",
      "   that: 0.2565\n",
      "   the: 0.1709\n",
      "   it: 0.1508\n",
      "  :: 0.1036\n",
      "   \": 0.0358\n",
      "\n",
      "🔍 Token step 8\n",
      "   the: 0.2524\n",
      "   it: 0.0770\n",
      "   \": 0.0546\n",
      "   there: 0.0364\n",
      "   Syria: 0.0242\n",
      "\n",
      "🔍 Token step 9\n",
      "   Syrian: 0.1489\n",
      "   United: 0.0661\n",
      "   government: 0.0499\n",
      "   situation: 0.0377\n",
      "   country: 0.0332\n",
      "\n",
      "🔍 Token step 10\n",
      "   government: 0.5813\n",
      "   people: 0.0613\n",
      "   regime: 0.0576\n",
      "   army: 0.0372\n",
      "   civil: 0.0318\n",
      "\n",
      "🔍 Token step 11\n",
      "   has: 0.2848\n",
      "   is: 0.2150\n",
      "  's: 0.0597\n",
      "   and: 0.0451\n",
      "   had: 0.0351\n",
      "\n",
      "🔍 Token step 12\n",
      "   been: 0.1316\n",
      "   not: 0.0515\n",
      "   \": 0.0377\n",
      "   no: 0.0333\n",
      "   taken: 0.0251\n",
      "\n",
      "🔍 Token step 13\n",
      "   responsible: 0.0623\n",
      "   unable: 0.0585\n",
      "   \": 0.0485\n",
      "   in: 0.0313\n",
      "   using: 0.0313\n",
      "\n",
      "🔍 Token step 14\n",
      "   for: 0.9359\n",
      "   and: 0.0177\n",
      "   in: 0.0081\n",
      "   to: 0.0078\n",
      "  ,: 0.0074\n",
      "\n",
      "🔍 Token step 15\n",
      "   the: 0.4324\n",
      "   a: 0.0567\n",
      "   its: 0.0236\n",
      "   an: 0.0168\n",
      "   more: 0.0139\n",
      "\n",
      "🔍 Token step 16\n",
      "   deaths: 0.0964\n",
      "   destruction: 0.0751\n",
      "   violence: 0.0344\n",
      "   death: 0.0313\n",
      "   attacks: 0.0313\n",
      "\n",
      "🔍 Token step 17\n",
      "   of: 0.7872\n",
      "   and: 0.1117\n",
      "   in: 0.0380\n",
      "  ,: 0.0261\n",
      "   at: 0.0036\n",
      "\n",
      "🔍 Token step 18\n",
      "   more: 0.1786\n",
      "   civilians: 0.0545\n",
      "   over: 0.0512\n",
      "   hundreds: 0.0512\n",
      "   thousands: 0.0330\n",
      "\n",
      "🔍 Token step 19\n",
      "   than: 0.9983\n",
      "   people: 0.0004\n",
      "   civilians: 0.0003\n",
      "   and: 0.0001\n",
      "   Syrian: 0.0000\n",
      "\n",
      "🔍 Token step 20\n",
      "   1: 0.1421\n",
      "   100: 0.0976\n",
      "   2: 0.0917\n",
      "   200: 0.0433\n",
      "   3: 0.0359\n",
      "\n",
      "📝 Final generated text:\n",
      "The United Nations has issued a statement on the situation in Syria, saying that the Syrian government has been responsible for the deaths of more than 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_with_token_probabilities(prompt, max_tokens=20):\n",
    "    \"\"\"Generate text token by token, displaying each step with probabilities.\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(f\"📝 Starting prompt: {prompt}\")\n",
    "    generated_text = prompt\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits  # Raw model outputs\n",
    "        \n",
    "        # Get probability distribution for next token\n",
    "        probs = torch.softmax(logits[0, -1, :], dim=-1)  \n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "\n",
    "        # Print top token choices\n",
    "        print(f\"\\n🔍 Token step {_+1}\")\n",
    "        for i in range(5):  # Show top 5 predictions\n",
    "            token = tokenizer.decode([sorted_indices[i].item()])\n",
    "            print(f\"  {token}: {sorted_probs[i].item():.4f}\")\n",
    "\n",
    "        # Select most probable token\n",
    "        next_token_id = sorted_indices[0].item()\n",
    "        next_token = tokenizer.decode([next_token_id])\n",
    "\n",
    "        # Append token to generated text\n",
    "        generated_text += next_token\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]]).to(device)], dim=1)\n",
    "\n",
    "        # Stop if end token is reached\n",
    "        if next_token == tokenizer.eos_token:\n",
    "            print(\"\\n🚫 End of text token reached.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n📝 Final generated text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "# Test the function\n",
    "generate_with_token_probabilities(\"The United Nations has issued a statement on\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef089eb8-2c9d-4933-8c83-68d81b52298b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679dfdc-7afe-42ef-a236-aa2d6c893a54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
